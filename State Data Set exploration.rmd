---
title: 'IMT 573: Problem Set 4 - Data Analysis'
author: "Saurabh Sharma"
date: 'Due: Tuesday, October 29, 2019'
output: pdf_document
---

<!-- This syntax can be used to add comments that are ignored during knitting process. -->

##### Collaborators: <!-- BE SURE TO LIST ALL COLLABORATORS HERE! -->

##### Instructions:

Before beginning this assignment, please ensure you have access to R and RStudio; this can be on your own personal computer or on the IMT 573 R Studio Server. 

1. Download the `problemset4.rmd` file from Canvas or save a copy to your local directory on RStudio Server. Open `problemset4.rmd` in RStudio and supply your solutions to the assignment by editing `problemset4.rmd`. 

2. Replace the "Insert Your Name Here" text in the `author:` field with your own full name. Any collaborators must be listed on the top of your assignment. 

3. Be sure to include well-documented (e.g. commented) code chucks, figures, and clearly written text chunk explanations as necessary. Any figures should be clearly labeled and appropriately referenced within the text. Be sure that each visualization adds value to your written explanation; avoid redundancy -- you do no need four different visualizations of the same pattern.

4.  Collaboration on problem sets is fun and useful, and we encourage it, but each student must turn in an individual write-up in their own words as well as code/work that is their own.  Regardless of whether you work with others, what you turn in must be your own work; this includes code and interpretation of results. The names of all collaborators must be listed on each assignment. Do not copy-and-paste from other students' responses or code.

5. All materials and resources that you use (with the exception of lecture slides) must be appropriately referenced within your assignment.  

6. Remember partial credit will be awarded for each question for which a serious attempt at finding an answer has been shown. Students are \emph{strongly} encouraged to attempt each question and to document their reasoning process even if they cannot find the correct answer. If you would like to include R code to show this process, but it does not run withouth errors you can do so with the `eval=FALSE` option.

```{r example chunk with a bug, eval=FALSE, include=FALSE}
a + b # these object dont' exist 
# if you run this on its own it with give an error
```

7. When you have completed the assignment and have **checked** that your code both runs in the Console and knits correctly when you click `Knit PDF`, rename the knitted PDF file to `ps4_ourLastName_YourFirstName.pdf`, and submit the PDF file on Canvas.

##### Setup

In this problem set you will need, at minimum, the following R packages.

```{r Setup, message=FALSE}
# Load standard libraries
library(tidyverse)
library(gridExtra)
library(dplyr)
library(data.table)

```

#### Problem 1: 50 States in the USA

In this problem we will use the `state` dataset, available as part of the R statistical computing platforms. This data is related to the 50 states of the United States of America. Load the data and use it to answer the following questions. 

##### (a) Describe the data and each variable it contains. Tidy the data, preparing it for a data analysis.

```{r}
?state
statedataset <- data(state)
#loading the state datasset

state_df <- as.data.frame(state.x77)

state_df_s <- setDT(state_df, keep.rownames = "state")[]
#Making a state column in the dataset

dim(state_df)

#now describe state data


```

The dataset is about the 50 states of US. The large states data is made up of several datasets:
- state.abb- character vector of 2-letter abbreviations for the state names
- state.area- numeric vector of state areas (in square miles)
- state.center- list with components named x and y giving the approximate geographic center of   each state
- state.division- factor giving state divisions
- state.name- character vector giving the full state names
- state.region- factor giving the region (Northeast, South, North Central, West) that each state  belongs to.
- state.x77- matrix with 50 rows and 8 columns telling about: Population, Income,  Illiteracy,Life Exp, Murder, HS Grad, Frost, Area

The dimensions of the state dataset are 50 rows and 8 columns having variables such as Population, Income, Illiteracy, Life Expectancy, Murder Rate, High Scholl Graduates, Frost and Area.

##### (b) Suppose you want to explore the relationship between a state's `Murder` rate and other characteristics of the state, for example population, illiteracy rate, and more. Begin by examining the bivariate relationships present in the data. Present and interpret numeric value to describe the linear relationships as well as plots to contextualize these numeric values. What does your analysis suggest might be important varibles to consider in building a model to explain variation in murder rates? Are linear relationships appropriate to assume for all bivariate relationships? Why or why not?

I would like to explore the bivariate relationship between murder rate and illiteracy and also murder rate and income. 
Let us try to understand the relationship between this variables by plotting the bivariate relationship of Murder Rate vs Illiteracy, and Murder Rate vs Income.


```{r}
#plotting illiteracy vs murder
ggplot(state_df_s, aes(x= Illiteracy, y= Murder )) + geom_point() +
  geom_smooth()
```

We can see that there is almost linear relationship betwwen Illiteracy and Murder Rate.

```{r murdervsincome}
#plotting income vs murder
ggplot(state_df_s, aes(x= Income, y= Murder )) + geom_point() +
  geom_smooth()
```

We can see that initially murder rate decreases with the increase in income but we can see a very strong outlier having income higher than 6000 and murder rate also quite high. We would try to inspect this outlier.

```{r outlier}
state_df_s %>% filter(Income > 6000)
```
This tells that the state of Alaska is an outlier having the highest income among all the states, still very high murder rate.

##### (c) Develop a new research question of your own that you can address using the `state` dataset. Clearly state the question you are going to address. Provide at least one visualization to support your exploration of this question. Discuss what you find in your exploration.

Question: Are the states with higher mean number of days having below freezing temperature (frost) sparsely populated?

We need to find population density, i.e.- number of people living per square mile. When we find the dataset, we find that the population mentioned is very small, it seems unreasonalbe that a state would have so less population. So we would take a multiplication of thousand, assuming the true population of a state  should be: 1000 times the mentioned population. We would add a column for population density, pd = (Population*1000)/ Area


```{r}
state_df_pd <- state_df_s %>% mutate(pd =
                                        (Population*1000)/Area
                                                               )

#We added another column pd for population density
```

We would plot the relationship between Population Density and number of days having below freezing point temperature in an year.

```{r pd_frostdays}
ggplot(state_df_pd, aes(x= Frost, y= pd )) + geom_point() +
  geom_smooth()
```

There are a few states having relatively higher population density which are distorting the best fit line. Let us try to find them.

```{r pdstates}
state_df_pd %>% filter(pd> 600)
```
We can see that these states have 100-150 days that get below freezing point temperature every year, still they are very high populated. There may be other reasons for that such as people habitating their at the time of industrialization or some other geographical advantage or historical importance.


#### Problem 2: Asking Data Science Questions: Crime and Educational Attainment

In Problem Set 3, you joined data about crimes and educational attainment. Here you will use this new combined dataset to examine questions around crimes in Seattle and the educational attainment of people
living in the areas in which the crime occurred. The combined state dataset is available on the course Canvas website (note: this will be available after all students submit Problem Set 3).

combinedCrimeDataset

```{r crimeandbeatsandcensus}
problemset3data <- read.csv("combinedCrimeDataset.csv",stringsAsFactors=FALSE)
#We loaded the data from problem set 3 here

problemset3data %>% distinct(county)
problemset3data %>% distinct(Neighborhood)

```


#### (a) Develop a Data Science Question

Develop your own question to address in this analysis. Your question should be specific and measurable, and it should be able to be addressed through a basic analysis of the dataset from Problem Set 3. This analysis must involve at least one hypothesis test. Clearly state what the question is and the suitable null and alternative hypotheses.

Question: What is the educational attainment trend in the 5 neighbothoods of highest occurance of crime and 5 neighbourhoods of lowest occurance of crime. The hypothesis that the probability of people having less educational level would be higher in the high crime areas than in the low crime areas.



#### (b) Describe and Summarize

Briefly summarize the dataset, describing what data exists and its basic properties. Comment on any issues that need to be resolved before you can proceed with your analysis. Provide descriptive statistics of variables of interest.


```{r}
problemset3data <- as.data.frame(problemset3data)

dim(problemset3data)
summary(problemset3data)
```

The dataframe "problemset3data" has 347980*48 dimension; and is about the King county of wsahington state. It tells what kind of crimes have been committed after 2011 in various police beats and neighbothoods of the King county. It also gives the educational attainment levels of police beats and neighborhoods. 

#### (c) Data Analysis

Use the dataset to provide empirical evidence to answer your question from part (a). Discuss your results. Provide at least one visualization to support your narrative. (NOTE: you will not be graded on whether you see statistically significant results but rather on your interpretation of findings)

We are going to find the probability of having less educational level in top 5 high crime areas and top 5 low crime areas, and compare them. The probablity should be higher in the high crime areas.

Let us try to find out the crime frequency for different neighborhoods and select top 5 and bottom 5 to see the difference of education among the 5 neighbouhoods of most crime and 5 neighborhoods of least crimes.

Let us analyse the data for 5 neighborhoods of most crimes.



```{r}
neighborhood_highest <- problemset3data %>% group_by(Neighborhood) %>%
        summarise(crimefrequency = n()) %>% arrange(desc(crimefrequency)) %>%
                       filter(rank(desc(crimefrequency))<=5)

#We can see the top 5 neighborhoods of highest crime frequencies
neighborhood_highest


```

We would subset our dataframe to selct the rows for only these 5 neighborhoods.

```{r subsetneighborhood}
subsetneighborhood <- subset(problemset3data, 
                          (problemset3data$Neighborhood %in%
                                                   neighborhood_highest$Neighborhood))
#The subsetneighborhood dataframe has only rows for our selected neighborhoods. 

#str(subsetneighborhood)
```
I want to divide the educational levels in three categories:
school- X10th_grade, X11th_grade, X12th_grade_no_diploma, high_school_diploma, ged_or_alternative_credential

college- some_college_less_than_1_year, some_college_1_year_or_more_years_no_degree, associates_degree, bachelors_degree

highedu- masters_degree, professional_school_degree, and doctorate_degree
I would add seperate columns for all these educational levels.

low_edu = no_schooling, nursery_school, kindergarten, X1st_grade, X2nd_grade, X3rd_grade,X4th_grade, X5th_grade,X6th_grade, X7th_grade,           X8th_grade, X9th_grade
```{r educationalcolumns}
neighborhood_edu<- subsetneighborhood %>% mutate(school= 
                                                 X10th_grade+ X11th_grade+ X12th_grade_no_diploma+ high_school_diploma+ ged_or_alternative_credential  ) %>%
  mutate(college = some_college_less_than_1_year+ some_college_1_or_more_years_no_degree+ associates_degree+ bachelors_degree ) %>% mutate(high_edu= masters_degree+ professional_school_degree+ doctorate_degree) %>% mutate(low_edu =
                                                                                                  no_schooling+ nursery_school+ kindergarten+ X1st_grade+ X2nd_grade+ X3rd_grade+ 
                                                                                                    X4th_grade+ X5th_grade+X6th_grade+ X7th_grade+ X8th_grade+ X9th_grade)
                                                                                                                                                                                                                                
```                                                                                                                                                  

We added seperate columns for number of people having school, college, higher or less education. We would now group by the neighbourhoods to find out the total sum of various educationsl levels in these neighborhoods.

```{r neighborhoodgorups}
neighborhoodgroups_highcrime<- neighborhood_edu %>% select(Neighborhood,school,college,
                                                 high_edu,low_edu) %>%
  group_by(Neighborhood) %>%
  summarise(school_level = sum(school), college_level = sum(college),
            high_edu_level = sum(high_edu), low_edu_level = sum(low_edu))


neighborhoodgroups_highcrime
```
We would use the gather function to bring the education levels in one column.
```{r neighborhood_edulevel}
neighborhood_edulevel_highcrime <- gather(neighborhoodgroups_highcrime,edu_level, value, "school_level","college_level", "high_edu_level","low_edu_level")
neighborhood_edulevel_highcrime
```

The probability of people having less education level in high crime areas=
```{r highcrime_eduprob}

highcrime_lesseduprob <-(3681744+2072738	+	1246824+1061876)/	(8821055+23113054+10187756+5802025+4742641+75755746+95087401+45835167+59874012+42069438+30651118+52529852+17299160+27078625	+19900591	+1182521+3681744+2072738+1246824	+1061876	)
highcrime_lesseduprob
```



    
    
We should now repeat all these steps for the 5 neighborhoods of least crimes. I would be adding the suffix lc(lease crime) infront of the dataframne names.

```{r nieghbothoodcrimefrequency_lc}
neighborhood_lc <- problemset3data %>% group_by(Neighborhood) %>%
        summarise(crimefrequency = n()) %>% arrange(crimefrequency) 

#We can see the top 5 neighborhoods of highest crime frequencies
neighborhood_lc
#from this neighborhood_lc, we select the top 5 neighborhoods except Unknown.

neighborhood_lc1 <- neighborhood_lc %>% filter(crimefrequency <1000 ) %>%
  filter(crimefrequency >20)
neighborhood_lc1
```
   These are the neighborhoods of least crimes.
   
   We would subset our dataframe to selct the rows for only these 5 neighborhoods.

```{r subsetneighborhood_lc}
subsetneighborhood_lc <- subset(problemset3data, 
                          (problemset3data$Neighborhood %in%
                                                   neighborhood_lc1$Neighborhood))

#subsetneighborhood_lc %>% distinct(Neighborhood)
#The subsetneighborhood dataframe has only rows for our selected neighborhoods. 

#str(subsetneighborhood)
```
  
  I want to divide the educational levels in three categories:
school- X10th_grade, X11th_grade, X12th_grade_no_diploma, high_school_diploma, ged_or_alternative_credential

college- some_college_less_than_1_year, some_college_1_year_or_more_years_no_degree, associates_degree, bachelors_degree

highedu- masters_degree, professional_school_degree, and doctorate_degree
I would add seperate columns for all these educational levels.

low_edu = no_schooling, nursery_school, kindergarten, X1st_grade, X2nd_grade, X3rd_grade,
                                                                                              X4th_grade, X5th_grade,X6th_grade, X7th_grade, X8th_grade, X9th_grade
                                                                                              
```{r educationalcolumns_lc}
neighborhood_edu_lc<- subsetneighborhood_lc %>% mutate(school= 
                                                 X10th_grade+ X11th_grade+ X12th_grade_no_diploma+ high_school_diploma+ ged_or_alternative_credential  ) %>%
  mutate(college = some_college_less_than_1_year+ some_college_1_or_more_years_no_degree+ associates_degree+ bachelors_degree ) %>% mutate(high_edu= masters_degree+ professional_school_degree+ doctorate_degree) %>% mutate(low_edu =
                                                                                                  no_schooling+ nursery_school+ kindergarten+ X1st_grade+ X2nd_grade+ X3rd_grade+ 
                                                                                                    X4th_grade+ X5th_grade+X6th_grade+ X7th_grade+ X8th_grade+ X9th_grade)
                                                                                                                                                                                                                                
                                                                                                                                                                                                
```                                                                                                                                                                                                    

We added seperate columns for number of people of having school, college, higher or less education. We would now group by the neighbourhoods to find out the total sum of various educationsl levels in these neighborhoods.

```{r neighborhoodgorups_lc}
neighborhoodgroups_lowcrime<- neighborhood_edu_lc %>% select(Neighborhood,school,college,
                                                 high_edu,low_edu) %>%
  group_by(Neighborhood) %>%
  summarise(school_level = sum(school), college_level = sum(college),
            high_edu_level = sum(high_edu), low_edu_level = sum(low_edu))


neighborhoodgroups_lowcrime
```
We would use the gather function to bring the education levels in one column.
```{r neighborhood_edulevel_lc}
neighborhood_edulevel_lowcrime <- gather(neighborhoodgroups_lowcrime,edu_level, value, "school_level","college_level", "high_edu_level","low_edu_level")
neighborhood_edulevel_lowcrime
```
 
 The probability of people having less education level in high crime areas=
```{r lowcrime_lesseduprob}

lowcrime_lesseduprob <- (12080+7688+2380+122388+15120)/(142730+80228+101745+362229+234720+687664+450988+1419075	+1924650	+790200+214314+143964+762790	+1044246	+228240+12080+7688+2380+122388+15120)
lowcrime_lesseduprob
```
 
 
 We would plot the graphs for educational levels in top 5 high crime areas and top 5 low crime areas. 

The following graph is for top 5 high crime area.
```{r plottinghighcrime}
ggplot(neighborhood_edulevel_highcrime, aes(x=edu_level, y= value)) +
  geom_bar(stat = "identity") + ggtitle("HighCrimeLocalty-EducationLevel")+ labs(x= "EduLevel", y="NumofPeople") + theme_bw() + theme(axis.text.x = element_text(angle = 90)) +
facet_wrap(~ neighborhood_edulevel_highcrime$Neighborhood)

```


The following graph is for top 5 low crime area.
```{r plottinglowcrime}
ggplot(neighborhood_edulevel_lowcrime, aes(x=edu_level, y= value) ) +
  geom_bar(stat = "identity") +ggtitle("LowCrimeLocalty-EducationLevel")+ labs(x= "EduLevel", y="NumofPeople") + theme_bw() + theme(axis.text.x = element_text(angle = 90))+
  
  facet_wrap(~ neighborhood_edulevel_lowcrime$Neighborhood)

```


We found that the probability of people having less educational level in high crime areas is 0.0152.

We found that the probability of people having less educational level in high crime areas is 0.0182.
These probabilities are not very different and it does not support the hypothesis that the probability of people having less ediucational level would be higher in high crime areas than low crime areas. Our graphs also do not support it.

However, in the first question of this assignment, we had shown that murder rate is directly proportional to illiteracy.


#### (d) Reflect and Question

Comment on the questions (and answers) in this analysis. Were you able to adequately answer your question? Is there additional data that would help provide a more clear picture of the problem you are analyzing?

I think that I was not quite able to adequaltely justify my question. The data I had did not tell me the total populatioin of every neighborhood. If I had the total population of each neighborhood, i could have told the different percentage of educational attainment levels of those neighborhoods. Without the total population of a neighborhood, I could only tell the number of people in different educational attainment buckets, instead of their percentage.

Moreover, the crime rate might have been higher because of other reasons than education. We are just checking in context of educational attainment, that does mean we can infer causality. There can be just a corelation.

#### Problem 3: Sampling with and without Replacement

In the following situations assume that half of the specified population wears glasses and the other half does not.

##### (a) Suppose you're sampling from a room with 10 people. What is the probability of sampling two people wearing glasses in a row when sampling with replacement? What is the probability when sampling without replacement?

As half of people wear glasses, the probability of each person wearing glasses is 0.5
With replacement:
Picking one person with glasses and choosing the second person also with glasses(with replacement are independent events. So the proability is


```{r}

#probabilty without replacement
probability_wo <-  0.5*0.5
probability_wo

#probabilty with replacement
probability_w_r <- 0.5*(4/9)
probability_w_r
```

##### (b)  Now suppose you're sampling from a stadium with 10,000 people. What is the probability of sampling two people wearing glasses in a row when sampling with replacement? What is the probability when sampling without replacement?


```{r}
#As half of the people wear glasses, the total number of glasses wearers is 5000. The probability of each person wearing glasses is 0.5. With replacement: probality is as following:
#probabilty without replacement
probability_wo <-  0.5*0.5
probability_wo

#Without replacement:
#The probability of 1st person wearning glasses is 0.5, the probality of second person wearing glasses is 4999/9999. So the combined probality is:

#probabilty with replacement
probability_w_r <- 0.5*(4999/9999)
probability_w_r
```

##### (c) We often treat individuals who are sampled from a large population as independent. Using your findings from parts (a) and (b), explain whether or not this assumption is reasonable.

In the part (a), we had sample size 10 and we found that there was a significant difference(0.25 and 0.222) in probabillities just because of replacing/not replacing one person.

In part (b), we had sampe size 10000 and we found that the probability did not change significantlya(0.25 and 0.2499) because of replacing/not replacing one person. 

This tells that in the large population, the combined probality of both person wearing glasses depended extremely less on the probability of 1st person wearing glasses, which portrays independence in large samples.